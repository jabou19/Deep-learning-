# ðŸ§  From Scratch to Insights: Feedforward Neural Networks in NumPy

**Training, Optimization, and Experiment Tracking with Weights & Biases (WandB)**

---

## ðŸ“˜ Overview

This project implements a **fully-connected Feedforward Neural Network (FFNN)** **from scratch** using **NumPy**,  
without relying on high-level deep learning libraries such as TensorFlow or PyTorch.

The model supports **forward and backward propagation**, **gradient-based optimization**, and **experiment tracking** via **Weights & Biases (WandB)**.

The primary goal is to help intermediate learners **understand the mathematical foundations** of deep learning  
and how modern frameworks like PyTorch perform these steps under the hood.

---

## ðŸŽ¯ Objectives

- Implement a complete FFNN using only NumPy
- Support configurable architecture and activation functions
- Implement **forward**, **backward**, and **parameter update** steps manually
- Train and evaluate on small datasets (e.g., Fashion-MNIST, CIFAR-10)
- Visualize and track experiments using **WandB**

---

## ðŸ§© Project Structure

```
DeepL_Project/
â”‚
â”œâ”€â”€ ffnn.py # Core network implementation (forward + backward + train)
â”œâ”€â”€ utils.py # (optional) Helper functions for activations, metrics
â”œâ”€â”€ train.py # Training & evaluation script
â”œâ”€â”€ Test.py # Simple test / toy dataset experiments
â”œâ”€â”€ data/ # (optional) Dataset loaders or local data
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ wandb/ # Local WandB run logs
â””â”€â”€ README.md # Project documentation
```
